{
  "hash": "80a716a28625b14f0993ee2145f3dd8a",
  "result": {
    "markdown": "---\ntitle: \"Shiny unit testing\" \nsubtitle: \"Part 1: Unit testing Shiny utility functions\"\nauthor: \"Martin Frigaard\"\ndate: \"2023-05-01\"\ncategories: [shiny, testing]\nimage: \"image.png\"\ntoc: true\ntoc-depth: 5\ntoc-title: 'Contents'\ntoc-location: \"left\"\n# code-block-border-left: true\ncode-block-bg: \"#f8f8f8\"\ncode-block-border-left: \"#e8e8e8\"\ncode-fold: show\ncode-summary: 'show/hide'\ncallout-icon: false\n\nfreeze: true\n\nexecute:\n  echo: true\n  warning: false\n  eval: false\n  collapse: true\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"packages\"}\nlibrary(testthat)\nlibrary(lobstr)\nlibrary(dplyr)\nlibrary(shiny)\nlibrary(covr)\n```\n:::\n\n\n\n\nThis post is the first in a series on testing shiny applications. I'll cover developing and testing a set of utility functions for a shiny app-package using [`testhat`](https://testthat.r-lib.org/). If you'd like to follow along, all the code I'll be using is contained in the [`utap` R package on GitHub](https://github.com/mjfrigaard/utap).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# renv::install(\"mjfrigaard/utap\")\nlibrary(utap)\n```\n:::\n\n\n## Shiny app-packages\n\nTesting the code in shiny app-packages can be more complicated than testing the code in a typical R package, because app-packages contain two types of code: \n\n1) **Application code**: functions designed to run the application  (i.e., the `ui` and `server` functions, modules, standalone app functions will a call to `shinyApp()`, etc.)\n\n2) **Everything else**: functions or code used for connecting to databases, uploading, importing, or manipulating data, building visualizations and/or tables, generating custom HTML layouts, etc. The non-application code and functions in app-packages are typically referred to as '[utility](https://engineering-shiny.org/build-app-golem.html?#submodules-and-utility-functions)' or '[helper](https://mastering-shiny.org/scaling-functions.html#file-organisation)' functions\n\nThese two types of code require different types of tests. Utility functions are usually accompanied by unit tests similar to the tests you'd find in a [standard R package](https://r-pkgs.org/testing-basics.html), while application code is tested using the [`shiny::testServer()`](https://shiny.posit.co/r/reference/shiny/1.7.0/testserver) function, or with the [`shinytest2` package](https://rstudio.github.io/shinytest2/). \n\nThis post will cover writing unit tests for a set of utility functions using [`testthat`](https://testthat.r-lib.org/) and [`covr`](https://covr.r-lib.org/). Any tips or time-savers I've found will be in green callout boxes:\n\n\n\n:::: {.callout-tip collapse='false'}\n\n## TIP!\n\n::: {style='font-size: 1.10em; color: #696969;'}\n\nThis is a tip!\n\n::: \n\n::::\n\n\n## What are unit tests?\n\n::: {.column-margin}\n![](testthat.png){width=40%}\n:::\n\n\n> \"*A unit test is a piece of code that invokes a unit of work and checks one specific end result of that unit of work. If the assumptions on the end result turn out to be wrong, the unit test has failed. A unit test’s scope can span as little as a method or as much as multiple classes.*\" - [The Art of Unit Testing, 2nd edition](https://www.manning.com/books/the-art-of-unit-testing-second-edition)\n\nI've found thinking of functions as 'units of work' and their desired behavior as an 'end results' provides a useful mental model during TDD. These terms also align nicely with the testing advice offered by  [`testthat`](https://r-pkgs.org/testing-design.html#sec-testing-design-principles): \n\n> *Strive to test each behaviour in one and only one test. Then if that behaviour later changes you only need to update a single test.*\n\nIn app-packages, the `testthat` package provides a comprehensive and flexible framework for performing unit tests. \n\n### testthat\n\nGet started with `testthat` in your app-package by running [`usethis::use_testthat()`](https://usethis.r-lib.org/reference/use_testthat.html). This function will create following files and folders: \n\n\n\n````default\ntests/\n  ├── testthat/\n  └── testthat.R\n````\n\n\nTo create new tests, run `usethis::use_test(\"utils_fun\")` (with `\"utils_fun\"` being the name of the function you'd like to test).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nusethis::use_test(\"utils_fun\")\n```\n:::\n\n\n````default\n✔ Setting active project to '/projects/apps/utap'\n✔ Writing 'tests/testthat/test-utils_fun.R'\n• Modify 'tests/testthat/test-utils_fun.R'\n````\n\n\n\n#### Test files\n\nNew test files are be created and opened from the `tests/testthat/` folder (with a `test-` prefix).\n\n-   The initial contents of a new test file contains the boilerplate code below:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code  code-fold=\"false\"}\n    test_that(\"multiplication works\", {\n      expect_equal(2 * 2, 4)\n    })\n    ```\n    :::\n\n    \n::: {.column-margin}\n\n![testthat test file](testthat-test-file.png){#fig-test-files width=60%}\n\nTest files\n:::\n\n#### Test structure \n\n`test_that()` sets the test \"scope\" or \"execution environment\", and encapsulates the expectations. \n\n-   Note the use of curly brackets after the `code` argument:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code  code-fold=\"false\"}\n    testthat::test_that(desc = \"description\", code = {\n      \n    })\n    ```\n    :::\n\n    \n\n::: {.column-margin}\n\n![tests](testthat-tests.png){#fig-tests width=90%}\n\n`testthat` test\n:::\n    \n#### Expectations\n\nTest expectations are the code that comes into direct contact with the *unit of work* and *end result* for each function. There are usually multiple expectations for any given function, so these are stored in **tests** (and the `desc` describes the test context for the set of expectations).\n\n-   All `testthat` expectations have an `expect_*` prefix:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code  code-fold=\"false\"}\n    testthat::expect_equal(object = 2 * 2, expected = 4)\n    ```\n    :::\n\n    \n::: {.column-margin}\n\n![expectations](testthat-expectation.png){#fig-expectations width=40%}\n\n`testthat` expectation\n:::\n\n\n#### Unit test development workflow\n\nI develop unit tests using the following workflow:  \n\n1. **Create the test file and R script**: I'll start by creating these files with `usethis::use_r()` and `usethis::use_test()`, even if I know the names of these files will likely change as I develop (see more below).\n\n2. **Define test context**: I use the test context (entered as a character string in the first argument of `testthat::test_that()`) to capture each \"unit of work\" for each function. I like to keep the test context short and sweet--the \"unit of work\" followed by \"works\" will suffice in most circumstances, unless there's a need for more specific details.\n\n3. **Write expectations:** These are the third item in the workflow, but conceptually these comes first--these are the \"end results\" I want from each function (i.e., compute a value, download a file, create a column, etc.). \n\nTests and expectations are grouped into test files based on their related objectives or goals, and should correspond to a similar `.R` file in the `R/` folder. \n\nWhile this workflow is probably not *technically* considered [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development), I *do* set up the tests before I start writing any code in the `R/` folder. This comes in handy if you're having to remind yourself where you stopped developing on a given project--I'll just run `devtools::test()` and the first failing test reminds me where to look.\n\n:::: {.callout-tip collapse='true'}\n## TIPS: Unit tests\n::: {style='font-size: 1.10em; color: #696969;'}\n\nThe advice on unit tests below (in **bold**) comes from [Effective Software Testing, 2022](https://www.manning.com/books/effective-software-testing). I've included descriptions of how `testthat` satisfies each recommendation:\n\n1) **Unit tests should be fast**: the text recommends unit tests take a '*couple of milliseconds*' to execute. `testthat` tests typically fall within this threshold (and provide time measurements to identify bottlenecks).\n\n2) **Unit tests are easy to control**: i.e., '*input values and the expected result value are easy to adapt or modify in the test*.' `testthat` expectations give us ample access to 1) the `expected` result and 2) the `observed` result. \n\n3) **Unit tests are easy to write**: i.e., '*do not require a complicated setup or additional work*'. When used combination with `usethis`, `testthat` unit tests can be set up, created, written, and run with a few lines of code: \n  \n   1.   `usethis::use_testthat()`     \n   2.   `usethis::use_test()`    \n   3.    `< write test >`    \n   4.    `testthat::test_file()`, `testthat::test_dir()`, or `devtools::test()`   \n  \n\n::: \n::::\n\n## App utility functions\n\nThe utility functions I'll be developing are designed to populate the `choices` argument for  `shiny::selectInput()`. For example, the `pull_numeric_cols()` function would 'pull' the column names from an input `data.frame` or `tibble` (the example below uses `palmerpenguins::penguins`):\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\npull_numeric_cols(palmerpenguins::penguins)\n```\n:::\n\n::: {.cell}\n\n```\n##      bill_length_mm       bill_depth_mm   flipper_length_mm \n##    \"bill_length_mm\"     \"bill_depth_mm\" \"flipper_length_mm\" \n##         body_mass_g                year \n##       \"body_mass_g\"              \"year\"\n```\n:::\n\n\nThe return values would be passed to an `updateSelectInput()` in the `server` to provide column names by `type` (i.e., numeric, binary, or categorical). These functions can be used to quickly group variables into groups for data visualizations. For example, binary variables can be mapped the color aesthetic (if using `ggplot2`), and custom functions can be created for other graph layers (i.e., facets).\n\nThe **unit of work** for each hypothetical `pull_[type]_cols()` function would be, \"*ingest a `data.frame` or `tibble` and identify columns by `type`*,\"  and their **end result** might be \"*return a (named) vector of column names by `type`.*\" In this case, `[type]` refers to the variable type (i.e., numeric, categorical, binary, etc.). See the hypothetical UI output example below:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# UI code\nshiny::selectInput(\n  inputId = ns(\"x\"),\n  label = \"X variable:\",\n  choices = NULL\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# server code\nshiny::observe({\n  num_vars <- pull_numeric_cols(df = data())\n  shiny::updateSelectInput(session,\n    inputId = \"x\",\n    choices = num_vars,\n    selected = num_vars[1])\n  }) |>\n  shiny::bindEvent(data(),\n    ignoreNULL = TRUE)\n```\n:::\n\n\nIn the example above, `pull_numeric_cols()` is passed a reactive dataset (`data()`), and the output is used to update the `selectInput()`.\n\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div class=\"form-group shiny-input-container\">\n<label class=\"control-label\" id=\"num_cols-label\" for=\"num_cols\">X variable:</label>\n<div>\n<select id=\"num_cols\" class=\"shiny-input-select\"><option value=\"bill_length_mm\" selected>bill_length_mm</option>\n<option value=\"bill_depth_mm\">bill_depth_mm</option>\n<option value=\"flipper_length_mm\">flipper_length_mm</option>\n<option value=\"body_mass_g\">body_mass_g</option>\n<option value=\"year\">year</option></select>\n<script type=\"application/json\" data-for=\"num_cols\" data-nonempty=\"\">{\"plugins\":[\"selectize-plugin-a11y\"]}</script>\n</div>\n</div>\n```\n:::\n:::\n\n\n<br>\n\n## Micro-iteration\n\nIn [R packages](https://r-pkgs.org/testing-basics.html#run-tests), micro-iteration is defined as, \"*the interactive phase where you initiate and refine a function and its tests in tandem.*\" If you're using [TDD](https://en.wikipedia.org/wiki/Test-driven_development?oldformat=true), you'll write the test first, then write the function to pass the test.\n\nThe first unit test I'll create is for `select_column_class()`, a function designed to return columns according to their `class()`. \n\n#### Function names \n\nComing up with names for functions can be challenging. I like to follow the [`tidyverse` style guide](https://style.tidyverse.org/syntax.html#object-names) and use short verbs as a prefix (`make_`, `get_`, `check_` etc.). I also like to use names that give 'future' me hints as to their behavior (i.e., `select_column_class()` imports and has similar behavior to `dplyr::select()`, while `pull_[type]_cols()` is more like `dplyr::pull()`)\n\n### Create test file \n\nI create the test file and function file in the **Console**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nusethis::use_test(\"select_column_class\")\n```\n:::\n\n\n````default\n✔ Setting active project to '/projects/apps/utap'\n✔ Writing 'tests/testthat/test-select_column_class.R'\n• Modify 'tests/testthat/test-select_column_class.R'\n````\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nusethis::use_r(\"select_column_class\")\n```\n:::\n\n\n````default\n• Modify 'R/select_column_class.R'\n````\n\n\n### Test context \n\nThe test context (entered as a character string in the first argument of `testthat::test_that()`) includes the \"unit of work\" for the function, followed by \"works\":\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntestthat::test_that(desc = \"select_column_class() is.tibble/is.data.frame works\", {\n  \n})\n```\n:::\n\n\nBefore I can start developing the `select_column_class()` function and it's tests, I'll need data. R comes with example data objects in the `datasets` package, but it's nice to have control over the data being used in your unit tests. I'll cover how to add test data available in your app-package.\n\n### Test data\n\nCreating test data is covered in [R packages](https://r-pkgs.org/testing-design.html#storing-test-data), but I'll summarize the key points: \n\n1. Test data (and other objects) can either be created within a test, or as a persistent [test fixture](https://r-pkgs.org/testing-advanced.html#sec-testing-advanced-concrete-fixture)  \n2. Test data fixtures should be stored in `tests/testthat/fixtures/<test_data.rds>`\n2. The code used to create any test data fixtures should be stored in the same folder with a `make_` prefix (i.e., `tests/testthat/fixtures/<make_test_data.R>`)\n\nThis is easier to picture with a demonstration: In the `tests/testthat/` folder, I'll create a new `fixtures` folder, and add a `make_testdata_col_class.R` file. \n\n\n\n````default\ntests/testthat/\n        └── fixtures/\n                └── make_testdata_col_class.R\n````\n\n\nIn `make_testdata_col_class.R`, I'll create `testdata_col_class` using the code below:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"test_data for test-select_column_class.R\"}\ntestdata_col_class <- tibble::tibble(\n log_var = c(TRUE, FALSE, TRUE),\n int_var = c(1L, 2L, 3L),\n dbl_var = c(1.1, 2.2, 3.3),\n chr_var = c(\"item:1\", \"item:2\", \"item:3\"),\n fct_var = factor(\n   c(\"group 1\", \"group 2\", \"group 3\"),\n   levels = c(\n     \"group 1\", \"group 2\", \"group 3\")),\n ord_var = factor(\n   c(\"level 1\", \"level 2\", \"level 3\"),\n   levels = c(\"level 1\", \"level 2\", \"level 3\"),\n   ordered = TRUE),\n list_var = list(\n   log_vec = c(TRUE, FALSE),\n   dbl_vec = c(1.1, 2.2),\n   chr_var = c(\"item:1\", \"item:2\")))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntestdata_col_class\n## # A tibble: 3 × 7\n##   log_var int_var dbl_var chr_var fct_var ord_var list_var  \n##   <lgl>     <int>   <dbl> <chr>   <fct>   <ord>   <named li>\n## 1 TRUE          1     1.1 item:1  group 1 level 1 <lgl [2]> \n## 2 FALSE         2     2.2 item:2  group 2 level 2 <dbl [2]> \n## 3 TRUE          3     3.3 item:3  group 3 level 3 <chr [2]>\n```\n:::\n\n\nI'll save `testdata_col_class` in `tests/testthat/fixtures/` as `testdata_col_class.rds`: \n\n\n\n````default\ntests/testthat/\n        └── fixtures/\n                ├── make_testdata_col_class.R\n                └── testdata_col_class.rds\n````\n\n\nTo load the data into my test, I'll add the following to the top of the test context: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntest_that(desc = \"select_column_class() is.tibble/is.data.frame works\", {\n  testdata_col_class <- readRDS(test_path(\"fixtures\", \"testdata_col_class.rds\"))\n  \n})\n```\n:::\n\n\n`testthat::test_path()` will load the data from the testing directory when I'm ready to run my test.  \n\n### Expectations\n\nIn `expect_equal()`, I'll verify the structure of the returned `object` is a `data.frame`/`tibble`. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntest_that(\"select_column_class() is.tibble/is.data.frame works\", {\n  testdata_col_class <- readRDS(test_path(\"fixtures\", \"testdata_col_class.rds\"))\n  # check tibble\n  testthat::expect_equal(\n    object =\n      select_column_class(\n        df = testdata_col_class,\n        class = \"???\") |>\n          tibble::is_tibble(),\n    expected = TRUE)\n})\n```\n:::\n\n\nWriting my expectations first forces me to make some decisions about what the arguments will be for the `select_column_class()` function (i.e., `df` and `class`). \n\n\n`select_column_class()` should return the columns according to their `class`, so I'll tests to verify the class of the return columns. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n  # check logical\n  testthat::expect_equal(\n    object =\n      select_column_class(\n        df = testdata_col_class,\n        class = \"log\") |>\n          lapply(is.logical) |> unlist() |> unique(),\n    expected = TRUE)\n```\n:::\n\n\n\nI try to write these in a way that's flexible (should the test data change in the future). \n\n:::: {.callout-tip collapse='true'}\n# **Expectation-Driven Development**\n\nWhether or not you decide to adopt Test-Driven Development, I strongly recommend writing test expectations while you're developing functions. It's a great opportunity to clarify a function's intended behaviors, arguments, and error/warning messages.\n\n::::\n\nAfter including tests for each class, I'll include a test for the error message from `select_column_class()` with `testthat::expect_error()`: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# test error type\ntestthat::test_that(\"select_column_class() type error\", {\n  testdata_col_class <- readRDS(test_path(\"fixtures\", \"testdata_col_class.rds\"))\n  # test type error\n  testthat::expect_error(\n    object = select_column_class(\n      df = testdata_col_class, \n      class = \"array\")\n  )\n})\n```\n:::\n\n\nWhen I've covered my intended 'end results' for `select_column_class()` (i.e., when it works and what happens when it doesn't), I'll write the function: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"select_column_class()\"}\nselect_column_class <- function(df, class) {\n\n  col_class <- function(df, class) {\n    switch(class,\n      log = dplyr::select(tibble::as_tibble(df), dplyr::where(is.logical)),\n      int = dplyr::select(tibble::as_tibble(df), dplyr::where(is.integer)),\n      dbl = dplyr::select(tibble::as_tibble(df), dplyr::where(is.double)),\n      chr = dplyr::select(tibble::as_tibble(df), dplyr::where(is.character)),\n      fct = dplyr::select(tibble::as_tibble(df), dplyr::where(is.factor)),\n      ord = dplyr::select(tibble::as_tibble(df), dplyr::where(is.ordered)),\n      list = dplyr::select(tibble::as_tibble(df), dplyr::where(is.list))\n    )\n  }\n\n  cl <- unique(class)\n  cl_check <- cl %nin% c(\"log\", \"int\", \"dbl\", \"chr\", \"fct\", \"ord\", \"list\")\n  if (any(cl_check)) {\n    cli::cli_abort(\"Invalid `class` argument. Must be one of:\\n\n          'log', 'int', 'dbl', 'chr', 'fct', 'ord', 'list'\")\n  }\n\n  col_list <- purrr::map(.x = class, .f = col_class, df = df)\n\n  df_cols <- purrr::list_cbind(col_list, size = nrow(df))\n\n  if (ncol(df_cols) < 1 || nrow(df_cols) < 1) {\n    df_cols <- structure(list(),\n      class = c(\"tbl_df\", \"tbl\", \"data.frame\"),\n      row.names = integer(0),\n      names = character(0)\n    )\n    return(df_cols)\n  } else {\n    return(df_cols)\n  }\n}\n```\n:::\n\n\n\n#### Recap: test data\n\nBelow is a summary of tips for adding data your tests. \n\n::: {#fig-unit_test_dep_data}\n\n![Unit test fixtures](unit_test_dep_data.png){#fig-unit_test_dep_data width=100%}\n\nUnit test fixtures \n:::\n\n## Mezzo-iteration\n\nThe `select_column_class()` will return a `tibble()` with the columns matching the `class` argument, but I'll also need an argument that allows me to adjust the returned object to a named character vector.  \n\nThat's the job of `get_column_class()`--this is a wrapper around `select_column_class()` with an additional `return_tbl` argument that, if `FALSE`, returns the column names as a named vector. \n\n#### Abstract syntax trees\n\nWhile developing R functions, I've found the `ast()` function from the [`lobstr` package](https://lobstr.r-lib.org/reference/ast.html) can be great for keeping track of nested function calls.\n\nFor example, `select_column_class()` has a nested `col_class()` function that isn't tested directly. So how do I make sure I'm keeping track of these nested functions in case they throw an error? I'll build an abstract function tree for the function in the [documentation](https://github.com/mjfrigaard/utap/blob/main/vignettes/utap.Rmd).\n\nBelow is the abstract syntax tree for `select_column_class()`:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n█─select_column_class \n└─█─col_class \n```\n:::\n:::\n\n\nThe tree above is simple--it only has two functions so far--but as packages grow these abstract displays become more important for tracking function calls (and tests!).\n\n### Combining tests\n\n`get_column_class()` calls `select_column_class()`, so I'll place both unit tests in the `tests/testthat/test-column_classes.R` file, and create the corresponding [`R/column_classes.R` file](https://github.com/mjfrigaard/utap/blob/main/R/column_classes.R)\n\nTo capture these nested functions visually, I'll include a function tree in a vignette or other source documentation. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n█─get_column_class \n└─█─select_column_class \n  └─█─col_class \n```\n:::\n:::\n\n\n\n\nI've combined `select_column_class()` and `get_column_class()` into a single file because I know every `pull_[type]_cols()` function would use `get_column_class()`. The following function tree captures this relationship. \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n█─get_column_class \n├─█─select_column_class \n│ └─█─col_class \n├─█─pull_binary_cols \n├─█─pull_facet_cols \n├─█─pull_cat_cols \n└─█─pull_numeric_cols \n```\n:::\n:::\n\n\n\n\n:::: {.callout-tip collapse='true'}\n# **Function file names**\n\nIn shiny app-packages, it's common to combine related functions (i.e., function families) into a single `.R` file with a prefix. \n\nFor example, a standalone app function combines the code that would otherwise sit in `ui.R` and `server.R`. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmyApp <- function() {\n  shiny::shinyApp(ui = \n      shiny::tagList(\n        # code from ui.R\n      ),\n      server = # code from server.R\n    )\n}\n```\n:::\n\n\n\nOther files that are automatically run with a standard shiny app (i.e., `global.R` or `helpers.R` files that used to load data, set themes/colors, etc.,) can be converted into functions and/or package files based on their purpose. For more information on organizing your `R/` folder, read [this section in R Packages](https://r-pkgs.org/code.html#sec-code-organising).\n\nAlso check out `golem::add_utils()` and `golem::add_fct()` for creating function files specific to shiny modules. \n::::\n\nIn the `test-column_classes.R` test file, I'll need more data for testing, but rather than create test data files for each test, I'll use test helpers to create the test data.\n\n### Test helpers\n\nTest helpers are stored in `tests/testthat/helper.R` and usually contain functions or code that 1) is too long to repeat with each test, and 2) doesn't take too much time or memory to run. Read more about test helpers  [here.](https://r-pkgs.org/testing-advanced.html#sec-testing-advanced-fixture-helper) \n\nI've created a [set of test helpers](https://github.com/mjfrigaard/utap/blob/main/tests/testthat/helper.R) in `utap` for creating different kinds of test data (because I'll be repeatedly defining columns with slightly different attributes). \n\nFor example, `col_maker()` can be used to create a `tibble` with columns based on the `col_type`, `size`, and `missing`:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncol_maker(col_type = c(\"log\", \"int\", \"dbl\", \n                       \"chr\", \"fct\", \"ord\"),\n          size = 6,\n          missing = TRUE)\n## # A tibble: 6 × 6\n##   log_var int_var dbl_var chr_var fct_var ord_var\n##   <lgl>     <int>   <dbl> <chr>   <fct>   <ord>  \n## 1 TRUE          1     0.1 item:1  group 1 level 1\n## 2 FALSE       135     3   item:2  group 2 level 2\n## 3 NA          269    NA   item:3  group 3 level 3\n## 4 TRUE        403     0.1 <NA>    <NA>    <NA>   \n## 5 FALSE        NA     3   item:1  group 1 level 1\n## 6 NA            1    NA   item:1  group 2 level 2\n```\n:::\n\n\nI can also create tibbles with custom columns using individual helper `_maker()` functions: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntibble::tibble(\n    log_var = log_maker(size = 2),\n    int_var = int_maker(size = 2),\n    dbl_var = dbl_maker(size = 2),\n    chr_var = chr_maker(size = 2),\n    list_var = list(fct_var = fct_maker(size = 3), \n                    ord_var = ord_maker(size = 3)),\n)\n## # A tibble: 2 × 5\n##   log_var int_var dbl_var chr_var list_var    \n##   <lgl>     <int>   <dbl> <chr>   <named list>\n## 1 TRUE          1     0.1 item: 1 <fct [3]>   \n## 2 FALSE         7     1   item: 2 <ord [3]>\n```\n:::\n\n\nThese helpers make it easier to iterate through the test expectations *and* function development, because `tibble`s like the one above can be developed *inside* each test. \n\n-   Below is an example for testing if `get_column_class()` will correctly identify the `logical` columns (for both return objects):\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code  code-fold=\"true\" code-summary=\"using test helpers\"}\n    testthat::test_that(\"get_column_class() logical\", {\n      # test logical class\n      testthat::expect_equal(\n        object = get_column_class(\n          # use test helper\n          df = col_maker(\n            col_type = c(\"log\", \"int\", \"dbl\", \"chr\"),\n              size = 6,\n              missing = FALSE,\n              lvls = 4),\n          class = \"log\") |>\n          unlist() |>\n          is.logical(),\n        expected = TRUE\n      )\n      # test logical names\n      testthat::expect_equal(\n        object = get_column_class(\n          # use test helper\n          df = col_maker(\n            col_type = c(\"log\", \"int\", \"dbl\", \"chr\"),\n              size = 6,\n              missing = FALSE,\n              lvls = 4),\n         class = \"log\",\n          return_tbl = FALSE\n        ),\n        expected = c(log_var = \"log_var\")\n      )\n    })\n    ```\n    :::\n\n\n\nWhen I'm confident with the `get_column_class()` function and it's tests, I'll save the test file and run `testthat::test_file()`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"show/hide get_column_class()\"}\nget_column_class <- function(df, class, return_tbl = TRUE) {\n  if (isFALSE(return_tbl)) {\n    col_types_df <- select_column_class(df, class = class)\n    nms <- names(col_types_df)\n    col_types <- purrr::set_names(nms)\n  } else {\n    col_types <- select_column_class(df, class = class)\n  }\n  return(col_types)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntestthat::test_file(\"tests/testthat/test-column_classes.R\")\n```\n:::\n\n\n````default\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 23 ]\n````\n\n\n### Test coverage \n\n*How many tests should I write?* \n\nAs function behavior grows in complexity, so does the number of expectations. In `testthat`, expectations are captured in tests, and code coverage measures the extent to which the tests in the `tests/testthat/` folder cover the possible execution paths of the functions in the `R/` folder (i.e. the package codebase). \n\nCode test coverage is a way to confirm that the unit tests are robust enough to verify that your code behaves as expected. In R packages, code coverage is discussed in the [testing chapter](https://r-pkgs.org/testing-design.html#sec-testing-design-coverage) using the [`covr` package](https://covr.r-lib.org/).\n\n### Check coverage interactively \n\nDuring development, check the code coverage of a test file with `devtools::test_coverage_active_file()`, or, if this function is being temperamental, use the combination of functions below from `covr`: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncovr::file_coverage(\n  source_files = \"R/column_classes.R\", \n  test_files = \"tests/testthat/test-column_classes.R\") |>\n  covr::report()\n```\n:::\n\n\n\nBelow is the output in the **Viewer** when `devtools::test_coverage_active_file()` is entered in the **Console**:\n\n:::: {.column-body-outset-right}\n\n::: {#fig-column_classes_covr}\n\n![Test coverage using `devtools::test_coverage_active_file()`](column_classes_covr.png){#fig-column_classes_covr width=100% fig-align=\"center\"}\n\nUnit test coverage interactively\n:::\n\n::::\n\n\nI can see from the output I don't have test coverage for the `select_column_class()` behavior when the `class` argument doesn't return any columns from `df`. The function is designed to return an empty `tibble` if this occurs: \n\n::: {#fig-column_classes_covr_incomplete}\n\n![Behavior not tested in `select_column_class()`](column_classes_covr_incomplete.png){#fig-column_classes_covr_incomplete width=100% fig-align=\"center\"}\n\n\nThe area in red is the untested portion of `select_column_class()`\n:::\n\nTo test this behavior, I'll write two expectations:\n\n-   The first expectation (`expect_s3_class()`) checks the class of the return object from `select_column_class()`:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code  code-fold=\"false\"}\n      # test class of empty tibble\n      testthat::expect_s3_class(\n        object = select_column_class(\n          df = col_maker(col_type = c(\"int\", \"dbl\"),\n                                      size = 6, \n                                      missing = FALSE),\n          class = \"log\"),\n        class = c(\"tbl_df\", \"tbl\", \"data.frame\"))\n    ```\n    :::\n\n\n-   The second expectation verifies there are zero columns in this return tibble:\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code  code-fold=\"false\"}\n      # test rows of empty tibble\n      testthat::expect_equal(\n        object = ncol(select_column_class(\n          df = col_maker(col_type = c(\"int\", \"dbl\"),\n                                      size = 6, \n                                      missing = FALSE),\n          class = \"log\")),\n        expected = 0L)\n    ```\n    :::\n\n\nAfter adding these tests to the `test-column_classes.R` test file, I'll run `testthat::test_file()` and `devtools::test_coverage_active_file()` again: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntestthat::test_file(\"tests/testthat/test-column_classes.R\")\n```\n:::\n\n\n````default\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 25 ]\n````\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndevtools::test_coverage_active_file()\n```\n:::\n\n\n:::: {.column-body-outset-right}\n\n::: {#fig-column_classes_covr_complete}\n\n![Test coverage using `devtools::test_coverage_active_file()`](column_classes_covr_complete.png){#fig-column_classes_covr_complete width=100% fig-align=\"center\"}\n\nComplete code coverage for `column_classes.R`\n:::\n\n::::\n\n100% is great, but uncommon. Striving for a high percentage of coverage is a good practice, it doesn't guarantee that the function always behaves as expected. Unit tests might execute a line of code, but still not catch a bug due to the design of the test (it's easy to have high coverage if the unit tests are shallow and don't check for any potential [edge cases](https://en.wikipedia.org/wiki/Edge_case)).\n\nI'll address code coverage again in the next section, but checking coverage regularly will help ensure function behaviors don't go overlooked. \n\n## Macro-iteration\n\nAfter developing the functions in `utap`, the files in the `R/` folder are organized into names [based on the](https://r-pkgs.org/code.html#sec-code-organising) '*main function and its supporting helpers*': \n\n\n\n````default\nR/\n├── column_classes.R\n├── pull_binary_cols.R\n├── pull_cat_cols.R\n├── pull_facet_cols.R\n├── pull_numeric_cols.R\n└── utils.R\n````\n\n\n### Test file organization\n\nThe `tests/testthat/` folder file names have identical names as the files in the `R/` folder.\n\n\n\n````default\ntests/testthat/\n        ├── test-column_classes.R\n        ├── test-pull_binary_cols.R\n        ├── test-pull_cat_cols.R\n        ├── test-pull_facet_cols.R\n        ├── test-pull_numeric_cols.R\n        └── test-utils.R\n````\n\n\n\n#### `R/utils.R`\n\nIt's common for R packages to have a general `R/utils.R` file that defines the 'utility' functions. This practice isn't discouraged in R Packages, but these files can become a catch-all for any functions that don't have a clear home (read more [here](https://rud.is/b/2018/04/08/dissecting-r-package-utility-belts/)).\n\nI've only stored the `%nin%` operator in `R/utils.R`, and it's test is shown below: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntestthat::test_that(\"%nin% works\", {\n  testthat::expect_false(\n    object = \"A\" %nin% LETTERS)\n  testthat::expect_false(\n    object = 1 %nin% 1:10)\n  testthat::expect_true(\n    object = 1 %nin% 2:10)\n})\n```\n:::\n\n\n\n### Test package \n\nWhen I've completed a set of test files, I can use `devtools::test()` to check if they're passing.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndevtools::test()\n```\n:::\n\n\n````default\nℹ Testing utap\n✔ | F W S  OK | Context\n✔ |        25 | column_classes                                                     \n✔ |        29 | pull_binary_cols                                                   \n✔ |         4 | pull_cat_cols                                                      \n✔ |        20 | pull_facet_cols                                                    \n✔ |         5 | pull_numeric_cols                                                  \n✔ |         3 | utils                                                              \n\n══ Results ═════════════════════════════════════════════════════════════════════\nDuration: 2.1 s\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 86 ]\n\n🎯 Your tests hit the mark 🎯\n````\n\n\nThe output above shows all tests are passing (and some helpful words of encouragement). As you can see, the number of tests correspond to the number of functions in each test file. \n\nFor example, `pull_binary_cols()` and `pull_facet_cols()` required additional internal functions to define their use:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n█─pull_binary_cols \n├─█─check_binary_vec \n│ ├─█─check_log_binary \n│ ├─█─check_int_binary \n│ └─█─check_fct_binary \n└─█─make_binary_vec \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n█─pull_facet_cols \n├─█─check_facet_vec \n│ ├─█─check_chr_facet \n│ └─█─check_fct_facet \n└─█─make_facet_vec \n```\n:::\n:::\n\n\nWheras `pull_cat_cols()` and `pull_numeric_cols()` map onto existing classes:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n█─pull_cat_cols \n├─█─is.factor \n└─█─is.character \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n█─pull_numeric_cols \n├─█─is.integer \n└─█─is.double \n```\n:::\n:::\n\n\n### Check coverage on build/install \n\nTo check the code coverage for the utap package, I can run `devtools::test_coverage()` to view the output in the **Viewer**. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ndevtools::test_coverage()\n```\n:::\n\n\n````default\nℹ Computing test coverage for utap\n````\n\n\n\n\n:::: {.column-body-outset-right}\n\n::: {#fig-utap_coverage}\n\n![Final test coverage for `utap` package](utap_coverage.png){#fig-utap_coverage width=100% fig-align=\"center\"}\n\n`devtools::test_coverage()`\n:::\n\n::::\n\nClicking on any of the **Files** will open the **Source** tab and give a summary like the one above from `devtools::test_coverage_active_file()`. I can also use `covr::package_coverage()` in the **Console** for simpler output:\n\n\n\n````default\nutap Coverage: 100.00%\nR/column_classes.R: 100.00%\nR/pull_binary_cols.R: 100.00%\nR/pull_cat_cols.R: 100.00%\nR/pull_facet_cols.R: 100.00%\nR/pull_numeric_cols.R: 100.00%\nR/utils.R: 100.00%\n````\n\n\n#### Other metrics \n\nSometimes it's interesting to view the relationship between function size and number of tests using the [`cloc` package.](https://github.com/hrbrmstr/cloc).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(cloc)\n```\n:::\n\n\n`cloc` stands for *Count Lines of Code*, and it's a rough metric used to gauge code complexity. It's simple, but apparently provides \"*just as much predictive power as more elaborate constructs like cyclomatic complexity.*\"[source](https://www.oreilly.com/library/view/software-design-x-rays/9781680505795/)\n\nBelow is a count of the lines of code in each file in the `R` folder: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ncloc::cloc_by_file(\"R\")\n```\n:::\n\n\n````default\n# A tibble: 8 × 6\n  source filename                language   loc blank_lines comment_lines\n  <chr>  <chr>                   <chr>    <int>       <int>         <int>\n1 R      \"R/pull_binary_cols.R\"  R           53           2            57\n2 R      \"R/pull_facet_cols.R\"   R           42           2            73\n3 R      \"R/column_classes.R\"    R           41           6            65\n4 R      \"R/pull_numeric_cols.R\" R           19           1            24\n5 R      \"R/pull_cat_cols.R\"     R           13           0            19\n6 R      \"R/utils.R\"             R            3           0             7\n7 R      \"R/utap-package.R\"      R            2           0             7\n8 R      \"\"                      SUM        173          11           252\n````\n\n\nThis output also confirms the relationship between lines of code and tests.\n\n## Recap\n\nThis post has been an introduction to unit testing utility functions in a shiny app-package. When I'm confident the utility functions are working, I'll start adding them into modules (and testing with `testServer()` or `shinytest2`). Files names can change a lot throughout the course of developing a shiny app-package, so it's helpful to adopt (or create) a naming convention.\n\nIf you're using the `golem` framework to develop your shiny app-package, the `utils_` and `fct_` prefixes are used to define two different types of [utility/helper functions](https://engineering-shiny.org/structuring-project.html#conventions-matter): \n\n1. `utils_` files contain '*small helper functions* and '*top-level functions defining your user interface and your server function*' \n\n2. `fct_` files contain '*the business logic, which are potentially large functions*...*the backbone of the application and may not be specific to a given module*'.\n\nThis particular file naming convention isn't required, but as with most conventions, it's better when someone else comes up with the standard (and I just have to adopt and implement it). And having and sticking to a naming convention is typically more important than the convention itself. \n\n<!--\n\n\n\n````default\ntest-column_classes.R:\n            │\n            └── select_column_class()\n                  │\n                  └── get_column_class() \n````\n\n\n````default\ntree/\n  │\n  └── get_column_class()\n        │     │\n        │     └── select_column_class()\n        │\n        ├── pull_binary_cols()\n        │\n        ├── pull_facet_cols()\n        │\n        ├── pull_cat_cols()\n        │\n        └── pull_numeric_cols()\n````\n\n\n````default\ntree/\n  │\n  └── get_column_class() # used in all pull_[type]_cols()\n        │     │\n        │     └── select_column_class()\n        │\n        ├── pull_binary_cols()\n        │        │\n        │        ├── check_binary_vec()\n        │        │      │\n        │        │      ├── check_log_binary()\n        │        │      ├── check_int_binary()\n        │        │      └── check_fct_binary()\n        │        │\n        │        └── make_binary_vec()\n        │\n        ├── pull_facet_cols() # custom definition\n        │        │\n        │        ├── check_facet_vec()\n        │        │      │\n        │        │      ├── check_chr_facet()\n        │        │      └── check_fct_facet()\n        │        │\n        │        └── make_facet_vec()\n        │\n        ├── pull_cat_cols() # is.character & is.factor\n        │\n        └── pull_numeric_cols() # is.integer & is.double\n````\n\n\n-->",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/selectize-0.15.2/css/selectize.bootstrap3.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/selectize-0.15.2/js/selectize.min.js\"></script>\n<script src=\"../../site_libs/selectize-0.15.2/accessibility/js/selectize-plugin-a11y.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}